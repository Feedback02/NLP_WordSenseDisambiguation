{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "auR9fbx1Roaj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning import LightningModule\n",
    "import json\n",
    "import torchmetrics\n",
    "from torchmetrics import F1Score\n",
    "#import evaluate\n",
    "import copy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from bidict import bidict\n",
    "#from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ssR7H7dL831"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, trainingDataPath, testDataPath, validationDataPath,coarseMapsDataPath):\n",
    "        self.trainingDataPath = trainingDataPath\n",
    "        self.testDataPath = testDataPath\n",
    "        self.validationDataPath = validationDataPath\n",
    "        self.coarseMapsDataPath = coarseMapsDataPath\n",
    "\n",
    "        self.idxToCluster={}\n",
    "        self.clusterToIdx={}\n",
    "\n",
    "        self.IdxToMeaning = {}\n",
    "        self.meaningToIdx = {}\n",
    "        \n",
    "    def readData(self, dataPath):\n",
    "        # Read the JSON file\n",
    "        with open(dataPath, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Some transformation so that we have data in the form {i:object{},i+1:object{},...}. where i is an index (0,...,N)\n",
    "        json_data = {k: json_data[k] for k in list(json_data)}\n",
    "        json_data = {index: json_data[k] for index,k in enumerate(list(json_data))}\n",
    "        return json_data\n",
    "        \n",
    "    def retrieveMeaningMaps(self):\n",
    "        # Read the JSON file\n",
    "        with open(self.coarseMapsDataPath, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        position_meaning = 1\n",
    "        for index, (key, list_meanings) in enumerate(json_data.items()):\n",
    "            for meaning in list_meanings:\n",
    "                #next(iter(meaning)) gives us the first element in a fast way\n",
    "                self.IdxToMeaning[position_meaning] = next(iter(meaning)) \n",
    "                self.meaningToIdx[ next(iter(meaning)) ] = position_meaning\n",
    "                position_meaning+=1\n",
    "                \n",
    "        #homonimyCluster_IdxToCluster = {index+1:key for index, (key, _) in enumerate(json_data.items()) }\n",
    "        #homonimyCluster_ClusterToIdx = {value:key for (key, value) in homonimyCluster_IdxToCluster.items() }\n",
    "\n",
    "        self.meaningToIdx[\"unk\"] =  0\n",
    "        self.IdxToMeaning[0] = \"unk\"\n",
    "        \n",
    "        return [ self.meaningToIdx, self.IdxToMeaning ]\n",
    "\n",
    "\n",
    "    def retrieveClusterMaps(self):\n",
    "        # Read the JSON file\n",
    "        with open(self.coarseMapsDataPath, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "                \n",
    "        self.idxToCluster = {index+1:key for index, (key, _) in enumerate(json_data.items()) }\n",
    "        self.clusterToIdx = {value:key for (key, value) in self.idxToCluster.items() }\n",
    "\n",
    "        self.clusterToIdx[\"unk\"] =  0\n",
    "        self.idxToCluster[0] = \"unk\"\n",
    "        \n",
    "        return [ self.clusterToIdx, self.idxToCluster ]\n",
    "\n",
    "\n",
    "    def retrieveClusterToMeaningsMaps(self):\n",
    "        # Read the JSON file\n",
    "        with open(self.coarseMapsDataPath, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "        idxClusterToIdxMeanings = {}\n",
    "        idxMeaningsToidxCluster = {}\n",
    "        for index, (key, list_meanings) in enumerate(json_data.items()):\n",
    "           idxClusterToIdxMeanings[ self.clusterToIdx[key] ] = [self.meaningToIdx[next(iter(meaning))] for meaning in list_meanings]\n",
    "           for meaning in list_meanings:\n",
    "               idxMeaningsToidxCluster[self.meaningToIdx[next(iter(meaning))]] = self.clusterToIdx[key]\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        return idxClusterToIdxMeanings,idxMeaningsToidxCluster\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def retrieveDataSet(self):\n",
    "        trainingData = self.readData(self.trainingDataPath)\n",
    "        testData = self.readData(self.testDataPath)\n",
    "        validationData = self.readData(self.validationDataPath)\n",
    "        #coarseMapsData = readJson(self, self.validationDataPath)\n",
    "\n",
    "        return [ trainingData, testData, validationData ] \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34lgyyUpI-tr",
    "outputId": "1696ca91-4aff-4ad2-8c0c-ef832c5fbe09"
   },
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "trainingDataPath_coarse = 'train_coarse_grained.json'\n",
    "testDataPath_coarse = 'test_coarse_grained.json'\n",
    "validationDataPath_coarse = 'val_coarse_grained.json'\n",
    "coarseMapsDataPath_coarse = 'coarse_fine_defs_map.json'\n",
    "\n",
    "\n",
    "trainingDataPath_fine = 'train_fine_grained.json'\n",
    "testDataPath_fine = 'test_fine_grained.json'\n",
    "validationDataPath_fine = 'val_fine_grained.json'\n",
    "\n",
    "manager_dataset_fine = LoadData(trainingDataPath_fine, testDataPath_fine, validationDataPath_fine,coarseMapsDataPath_coarse)\n",
    "\n",
    "\n",
    "manager_dataset_coarse = LoadData(trainingDataPath_coarse, testDataPath_coarse, validationDataPath_coarse,coarseMapsDataPath_coarse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainingData_coarse, testData_coarse, validationData_coarse  = manager_dataset_coarse.retrieveDataSet()\n",
    "\n",
    "trainingData_fine, testData_fine, validationData_fine  = manager_dataset_fine.retrieveDataSet()\n",
    "\n",
    "\n",
    "clusterToIdx, idxToCluster = manager_dataset_coarse.retrieveClusterMaps()\n",
    "meaningToIdx, IdxToMeaning = manager_dataset_coarse.retrieveMeaningMaps()\n",
    "\n",
    "idxClusterToIdxMeanings,idxMeaningsToidxCluster = manager_dataset_coarse.retrieveClusterToMeaningsMaps()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YpTklVNrSLUM"
   },
   "outputs": [],
   "source": [
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer,homonimyClusterClusterToIdx):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.homonimyClusterClusterToIdx = homonimyClusterClusterToIdx\n",
    "        self.data = data\n",
    "        #self.data = {k: data[k] for k in list(data)[:]}\n",
    "        #self.data1 = {index: batch[k] for index,k in enumerate(list(batch)[:4])}\n",
    "\n",
    "    def __len__(self):\n",
    "        #return max(len(sub_array[\"lemmas\"]) for _,sub_array in self.data.items())\n",
    "        return len(self.data)\n",
    "        \n",
    "    def is_int_convertible(self, variable):\n",
    "        try:\n",
    "            int(variable)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "            \n",
    "    def __getitem__(self, data_index):\n",
    "      batch = self.data[data_index]\n",
    "      inputs_tokenized = self.tokenizer.batch_encode_plus(\n",
    "            [batch[\"lemmas\"]],\n",
    "            add_special_tokens=True,  # Disable adding [CLS] and [SEP] tokens\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "      output_tokenized = copy.copy(batch[\"senses\"])\n",
    "\n",
    "# convert senses in the correspondig number\n",
    "      for (key, clusters) in output_tokenized.items():\n",
    "        for index,cluster in enumerate(clusters):\n",
    "          if not self.is_int_convertible(cluster):\n",
    "              try:\n",
    "                output_tokenized[key][index] = self.homonimyClusterClusterToIdx[cluster]\n",
    "              except KeyError:\n",
    "                # key \"cluster\" does not exist (last element)\n",
    "                output_tokenized[key][index] = self.homonimyClusterClusterToIdx[\"unk\"]\n",
    "              \n",
    "      candidates = copy.copy(batch[\"candidates\"])\n",
    "# convert candidates in the correspondig number\n",
    "      for (key, clusters) in candidates.items():\n",
    "        for index,cluster in enumerate(clusters):\n",
    "            \n",
    "          #if we cannot convert it in int it means that cluster is a string\n",
    "            if not self.is_int_convertible(cluster):\n",
    "              try:\n",
    "                candidates[key][index] = self.homonimyClusterClusterToIdx[cluster]\n",
    "              except KeyError:\n",
    "                # key \"cluster\" does not exist (last element)\n",
    "                candidates[key][index] = self.homonimyClusterClusterToIdx[\"unk\"]\n",
    "              \n",
    "              \n",
    "    # for every sense we have its number\n",
    "      inputs_tokenized[\"senses\"]= output_tokenized\n",
    "\n",
    "    #indexes in which i need to assign label =! -100\n",
    "      target_word_positions = [ inputs_tokenized.word_ids(batch_index=0)[idx] for idx in [ int(key) for key,_ in inputs_tokenized[\"senses\"].items() ]  ]\n",
    "      target_word_idx = [ int(key) for key,_ in inputs_tokenized[\"senses\"].items() ]\n",
    "      target_label = [ value[0] for key,value in inputs_tokenized[\"senses\"].items() ]\n",
    "\n",
    "      \n",
    "    \n",
    "      labels = []\n",
    "      idx_label = 0\n",
    "      #the position of the target token after tokenization\n",
    "      position_target_token = [] \n",
    "        \n",
    "      for iterator, idx in enumerate(inputs_tokenized.word_ids()):\n",
    "            if idx in target_word_idx:\n",
    "              labels.append(target_label[idx_label])\n",
    "              position_target_token.append(iterator)\n",
    "              idx_label+=1\n",
    "              target_word_idx.remove(idx)\n",
    "            else:\n",
    "              labels.append(-100)\n",
    "\n",
    "    #inputs_tokenized[\"labels\"] = labels\n",
    "      candidates_list = []\n",
    "      for iterator,(_,value) in enumerate(candidates.items()):\n",
    "              candidates_list.append( torch.tensor(value) )\n",
    "\n",
    "\n",
    "      #candidates = { position_target_token[iterator]: value for iterator,(_,value) in enumerate(candidates.items()) }\n",
    "      #candidates_list = torch.cat([ torch.tensor(elements) for elements in candidates_list.values()]).view(-1)\n",
    "      candidates_list = torch.cat(candidates_list).view(-1)\n",
    "      #candidates_list_padded = nn.ConstantPad1d((0, 50 - len(candidates_list)),0)(candidates_list)\n",
    "      #batch[\"input_ids\"] = inputs_tokenized[\"input_ids\"]\n",
    "      #batch[\"attention_mask\"] = inputs_tokenized[\"attention_mask\"]\n",
    "      #batch[\"labels\"] = torch.tensor(labels)\n",
    "      #batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "        # \"position_target_token\": nn.ConstantPad1d((0, 512 - len(position_target_token)),-1)(torch.tensor([position_target_token],dtype=torch.int16))\n",
    "      #print(candidates)\n",
    "      return {\"input_ids\" : inputs_tokenized[\"input_ids\"].squeeze(), \"attention_mask\": inputs_tokenized[\"attention_mask\"].squeeze(),\n",
    "              \"labels\":torch.tensor(labels, dtype=torch.long), \"index\": nn.ConstantPad1d((0, 512 - 1),-1)(torch.tensor([data_index])),\n",
    "                \"candidates\": nn.ConstantPad1d((0, 512 - len(candidates_list)),-1)(candidates_list) } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vpfrNsyIXj4J"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CoarseClassifier(LightningModule):\n",
    "    def __init__(self, num_classes, model_name_or_path,type_problem, learning_rate=2e-5):\n",
    "        super(CoarseClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.bert = AutoModel.from_pretrained(model_name_or_path, output_hidden_states=True)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        #self.bert.config.hidden_size\n",
    "        self.classifier1 = nn.Linear(self.bert.config.hidden_size, num_classes )\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.f1score = F1Score(task=\"multiclass\",num_classes=self.num_classes ,ignore_index=-100)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids, attention_mask) #,pooled_output\n",
    "        avarage_hidden_state = torch.stack(bert_output.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "        #hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        #pooled_output = hidden_state[:, 0]  # (bs, dim\n",
    "        #candidates = candidates.unsqueeze(1).expand(-1, 512, -1)\n",
    "        #output = torch.cat((avarage_hidden_state, candidates), dim=2)\n",
    "        #output = self.dropout(avarage_hidden_state)\n",
    "        output = self.classifier1(avarage_hidden_state)\n",
    "    \n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #get the candidates (ignore all the values -1)\n",
    "        #candidates  = torch.stack([candidate[:torch.nonzero(candidate == -1)[0][0]] for candidate in batch['candidates']])\n",
    "        #candidates  = torch.stack([candidate for candidate in batch['candidates']])\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        self.log('train_loss', loss,prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #candidates  = torch.stack([candidate for candidate in batch['candidates']])\n",
    "        #get the candidates (ignore all the values -1)\n",
    "\n",
    "        \n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        self.log('val_loss', loss,prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FineClassifier(LightningModule):\n",
    "    def __init__(self,CoarseClassifier,num_classes_coarse, num_classes_fine, model_name_or_path,type_problem, learning_rate=2e-5):\n",
    "        super(FineClassifier, self).__init__()\n",
    "        \n",
    "        self.num_classes_coarse = num_classes_coarse\n",
    "        self.num_classes_fine = num_classes_fine\n",
    "        self.bert = CoarseClassifier\n",
    "        # Freeze the base model's parameters\n",
    "        for param in self.bert.parameters():\n",
    "            self.bert.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        #self.bert.config.hidden_size\n",
    "        self.classifier1 = nn.Linear(num_classes_coarse, num_classes_fine )\n",
    "        #+1 because we have unknown label\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.f1score = F1Score(task=\"multiclass\",num_classes=self.num_classes ,ignore_index=-100)\n",
    "        #self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids, attention_mask) #,pooled_output\n",
    "        #avarage_hidden_state = torch.stack(bert_output.hidden_states[-5:], dim=0).sum(dim=0)\n",
    "        #hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        #pooled_output = hidden_state[:, 0]  # (bs, dim\n",
    "        #candidates = candidates.unsqueeze(1).expand(-1, 512, -1)\n",
    "        #output = torch.cat((avarage_hidden_state, candidates), dim=2)\n",
    "        #output = self.dropout(bert_output)\n",
    "        #output = self.dropout(bert_output)\n",
    "        output = self.classifier1(bert_output)\n",
    "    \n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #get the candidates (ignore all the values -1)\n",
    "        #candidates  = torch.stack([candidate[:torch.nonzero(candidate == -1)[0][0]] for candidate in batch['candidates']])\n",
    "        #candidates  = torch.stack([candidate for candidate in batch['candidates']])\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        self.log('train_loss', loss,prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #candidates  = torch.stack([candidate for candidate in batch['candidates']])\n",
    "        #get the candidates (ignore all the values -1)\n",
    "\n",
    "        \n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        self.log('val_loss', loss,prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.training_step(trainDataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4ZDWS9y0XlV4"
   },
   "outputs": [],
   "source": [
    "# Save the trained model in the current path\n",
    "current_path = os.getcwd()\n",
    "\n",
    "model_path = os.path.join(current_path, \"saved_model\")\n",
    "model_name_or_path = 'kanishka/GlossBERT'\n",
    "#model_name_or_path = \"prajjwal1/bert-mini\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "batch_size = 16 #prima era 16\n",
    "#batch = {k: train_data[k] for k in list(train_data)[:4]}\n",
    "\n",
    "\n",
    "trainDataset_coarse = TokenClassificationDataset( trainingData_coarse, tokenizer, clusterToIdx)\n",
    "testDataset_coarse = TokenClassificationDataset( testData_coarse, tokenizer, clusterToIdx)\n",
    "valDataset_coarse = TokenClassificationDataset( validationData_coarse, tokenizer, clusterToIdx)\n",
    "\n",
    "trainDataset_fine = TokenClassificationDataset( trainingData_fine, tokenizer, meaningToIdx)\n",
    "testDataset_fine = TokenClassificationDataset( testData_fine, tokenizer, meaningToIdx)\n",
    "valDataset_fine = TokenClassificationDataset( validationData_fine, tokenizer, meaningToIdx)\n",
    "\n",
    "\n",
    "\n",
    "trainLoader_coarse = DataLoader(trainDataset_coarse, batch_size=batch_size, shuffle=False)\n",
    "testLoader_coarse = DataLoader(testDataset_coarse, batch_size=batch_size, shuffle=False)\n",
    "valLoader_coarse = DataLoader(valDataset_coarse, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainLoader_fine = DataLoader(trainDataset_fine, batch_size=batch_size, shuffle=False)\n",
    "testLoader_fine = DataLoader(testDataset_fine, batch_size=batch_size, shuffle=False)\n",
    "valLoader_fine = DataLoader(valDataset_fine, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159\n",
      "4477\n"
     ]
    }
   ],
   "source": [
    "num_classes_coarse = len(clusterToIdx)\n",
    "num_classes_fine = len(meaningToIdx) \n",
    "print(num_classes_coarse)\n",
    "print(num_classes_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVZ8D9nLxfOU",
    "outputId": "92229594-59b7-42d7-bd18-6b95b7d93fa3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kanishka/GlossBERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at kanishka/GlossBERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier_coarse = CoarseClassifier(num_classes=num_classes_coarse, model_name_or_path=model_name_or_path,type_problem=\"coarse\")\n",
    "\n",
    "classifier_coarse = classifier_coarse.load_from_checkpoint(os.path.join(model_path, \"model7.ckpt\"),num_classes=num_classes_coarse, model_name_or_path=model_name_or_path,type_problem=\"coarse\")\n",
    "\n",
    "\n",
    "classifier_fine = FineClassifier(classifier_coarse,num_classes_coarse= num_classes_coarse, num_classes_fine=num_classes_fine, model_name_or_path=model_name_or_path,type_problem=\"fine\")\n",
    "\n",
    "\n",
    "classifier_fine = classifier_fine.load_from_checkpoint(os.path.join(model_path, \"model7_fine.ckpt\"),CoarseClassifier=classifier_coarse,num_classes_coarse= num_classes_coarse, num_classes_fine=num_classes_fine, model_name_or_path=model_name_or_path,type_problem=\"fine\")\n",
    "                                                                                                                                       \n",
    "                                                                                                                                       \n",
    "                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=5,accelerator='gpu') #prima era 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\OneDrive\\Desktop\\nlp\\hmw2\\nlp2\\nlp2\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | bert        | CoarseClassifier | 111 M \n",
      "1 | dropout     | Dropout          | 0     \n",
      "2 | classifier1 | Linear           | 9.7 M \n",
      "-------------------------------------------------\n",
      "120 M     Trainable params\n",
      "0         Non-trainable params\n",
      "120 M     Total params\n",
      "483.251   Total estimated model params size (MB)\n",
      "C:\\Users\\amine\\OneDrive\\Desktop\\nlp\\hmw2\\nlp2\\nlp2\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0126f34f139b4d58911ba26c6f2c17d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(classifier_fine, train_dataloaders = trainLoader_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KIAqwR7NML4C"
   },
   "outputs": [],
   "source": [
    "#trainer.save_checkpoint(os.path.join(model_path, \"model7_fine.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineClassifier(\n",
       "  (bert): CoarseClassifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (classifier1): Linear(in_features=768, out_features=2159, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (classifier1): Linear(in_features=2159, out_features=4477, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_fine.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoarseClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (classifier1): Linear(in_features=768, out_features=2159, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_coarse.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_token(labels):\n",
    "    #get the position of target token AFTER tokenization\n",
    "    result = []\n",
    "    for iterator, label in enumerate(labels):\n",
    "        if label != -100:\n",
    "            #result.append({iterator:label})\n",
    "            result.append(iterator)\n",
    "    return result\n",
    "#original_data can be testData, trainingData or validationData\n",
    "def evaluate_model(classifier, loader, original_data):\n",
    "    # Set the model to evaluation mode\n",
    "    classifier.eval()\n",
    "    result_labels_target = []\n",
    "    result_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate( loader ),desc=\"Batch\", leave=False) as data:\n",
    "            for step, samples in data:\n",
    "                index = [ index_data[0] for index_data in samples['index'] ]\n",
    "\n",
    "                result_prediction = classifier(samples['input_ids'].cuda(),samples['attention_mask'].cuda())\n",
    "                #result_prediction = [ result[0] for result in result_prediction ]\n",
    "                sample_non_tokanized = [ original_data[ index_data.tolist() ] for index_data in index ] \n",
    "                \n",
    "                position_token = [get_position_token(labels) for labels in samples['labels'] ]\n",
    "\n",
    "                for idx_sample in range(len(sample_non_tokanized)):\n",
    "                    #print(position_token[idx_sample])\n",
    "                    #print(result_prediction[idx_sample][0])\n",
    "                    predicted_labels_idx = []\n",
    "                    for iterator, (target_token_idx, candidates) in enumerate(sample_non_tokanized[idx_sample][\"candidates\"].items()):\n",
    "                            #print(candidates)\n",
    "                            # Get the predicted labels considering only the candidates\n",
    "                            predicted_labels_idx.append(torch.argmax( \n",
    "                                torch.tensor([ result_prediction[idx_sample][ position_token[idx_sample][iterator] ][target_prob_idx] for target_prob_idx in candidates ]), dim=-1) )\n",
    "                    labels_target = [ value[0] for key,value in sample_non_tokanized[idx_sample][\"senses\"].items() ]\n",
    "                    predicted_labels = [ candidates[predicted_labels_idx[i]] for i,(key,candidates) in enumerate(sample_non_tokanized[idx_sample][\"candidates\"].items()) ]\n",
    "                    result_labels_target.append(labels_target)\n",
    "                    result_predicted_labels.append(predicted_labels)\n",
    "                    \n",
    "    result_labels_target =  [element for sublist in result_labels_target for element in sublist]\n",
    "    result_predicted_labels =  [element for sublist in result_predicted_labels for element in sublist]\n",
    "    \n",
    "    return result_labels_target,result_predicted_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_labels_target_fine,result_predicted_labels_fine = evaluate_model(classifier_fine,testLoader_fine, testData_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8508)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1Score(task=\"multiclass\",num_classes=num_classes_fine)(torch.tensor(result_labels_target_fine),torch.tensor(result_predicted_labels_fine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Coarse grained - section 2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_labels_target_coarse,result_predicted_labels_coarse = evaluate_model(classifier_coarse,testLoader_coarse, testData_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9403)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1Score(task=\"multiclass\",num_classes=num_classes_coarse)(torch.tensor(result_labels_target_coarse),torch.tensor(result_predicted_labels_coarse))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d74202226504670ba54361d09c2d15f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "3fac6a95145749d2a3a945423f478505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa7d9e9490304cce8535634ca3dd51a9",
      "placeholder": "​",
      "style": "IPY_MODEL_6961fb03ed5641779024aefe2052e301",
      "value": "Epoch 0:  38%"
     }
    },
    "4fb740f3957348e1a6022cf64375f614": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54918b6452544215aac8cf82c7421374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fac6a95145749d2a3a945423f478505",
       "IPY_MODEL_5f278dafae0f48ed9b7e4570babeac3e",
       "IPY_MODEL_c91bd9e3018044d8bb4374c12a36571c"
      ],
      "layout": "IPY_MODEL_0d74202226504670ba54361d09c2d15f"
     }
    },
    "5f278dafae0f48ed9b7e4570babeac3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b220e2e24072484e9010d19046885fc4",
      "max": 1543,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f536136e09284f6a8f795191696ab48c",
      "value": 587
     }
    },
    "6961fb03ed5641779024aefe2052e301": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa7d9e9490304cce8535634ca3dd51a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b220e2e24072484e9010d19046885fc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c91bd9e3018044d8bb4374c12a36571c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fb740f3957348e1a6022cf64375f614",
      "placeholder": "​",
      "style": "IPY_MODEL_dcec3fd920cf496cbf5cc6d8cbcca615",
      "value": " 587/1543 [4:28:24&lt;7:17:08, 27.44s/it, v_num=6]"
     }
    },
    "dcec3fd920cf496cbf5cc6d8cbcca615": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f536136e09284f6a8f795191696ab48c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
